{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Project Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import sktime\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from prophet import Prophet\n",
    "\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding basic concepts in Time Series\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 From White Noise to ACF plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Simulate a white noise for 200 observations and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "# generate white noise for 200 observations and plot it\n",
    "white_noise = np.random.normal(0, 1, 200)\n",
    "plt.plot(white_noise)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a white noise with mean = 4 and sd = 2. Then use the arima function to estimate the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a white noise with mean 4 and sd 2 then use the arima function to estimate the parameters\n",
    "white_noise = np.random.normal(4, 2, 200)\n",
    "ARIMA(white_noise, order=(1, 0, 0)).fit().summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explain with your own words what a random walk is (minimum 100 words). Explain with your own words what stationarity means for a time series.\n",
    "\n",
    "*A random walk is a time series where the next value is dependent on the previous value. A white noise is a time series where the next value is independent of the previous value.*\n",
    "\n",
    "*A stationary time series is a time series where the mean, variance and autocorrelation are constant over time.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create a random walk series, plot it, calculate the first difference series and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_walk = np.cumsum(np.random.normal(0, 1, 200))\n",
    "plt.plot(random_walk)\n",
    "plt.plot(np.diff(random_walk))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Generate WN drift data, convert it to a random walk and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_noise_drift = np.cumsum(np.random.normal(0, 1, 200)) + (np.arange(200) / 10)\n",
    "plt.plot(white_noise_drift)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ACF on the white noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(white_noise, lags=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What’s the characteristic of a white noise ACF ?\n",
    "\n",
    "*The ACF of a white noise is 0 for all lags. Moreover, they are never statistically significant.*\n",
    "\n",
    "Perform a Ljung-Box Test. Command : Box.test in R, ljung is an option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acorr_ljungbox(white_noise, lags=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ARMA models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the arima.sim function (or Python equivalent) to generate time series based on the autoregressive model, with slopes comprised between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_sim = ArmaProcess(ar = [0.3, -0.25]).generate_sample(nsample=200)\n",
    "plt.plot(arima_sim)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe ?\n",
    "\n",
    "*The ACF of the random walk is not 0 for all lags. Moreover, they are statistically significant. also, the lags are positively correlated.*\n",
    "\n",
    "Plot them, and the acf functions along with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(arima_sim, lags=20).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with the moving average model. What do you observe ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_sim = ArmaProcess(ma = [0.3, -0.25]).generate_sample(nsample=200)\n",
    "plt.plot(arima_sim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(arima_sim, lags=20).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe ?\n",
    "\n",
    "*The ACF of the MA simulation is not 0 for all lags. Moreover, they are statistically significant. also, the lags are negatively correlated.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Contrast AR(1) and AR(2) models. How do they differ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR(1)\n",
    "arima_sim = ArmaProcess(ar = [0.3, -0.25]).generate_sample(nsample=200)\n",
    "plt.plot(arima_sim)\n",
    "\n",
    "\n",
    "# AR(2)\n",
    "arima_sim = ArmaProcess(ar = [0.3, -0.25, 0.3]).generate_sample(nsample=200)\n",
    "plt.plot(arima_sim)\n",
    "\n",
    "plt.legend(['AR(1)', 'AR(2)'])\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*They differ by how the lags are correlated. The AR(2) model is more correlated to its 2 previous lags rather than the AR(1) model which is more correlated to only its first lag.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the difference between an Autocorrelation Function and a Partial autocorrelation Function ? (Min. 150 words)\n",
    "\n",
    "*The ACF is the correlation between the time series and its lags. The PACF is the correlation between the time series and its lags, while controlling for the effect of the intermediate lags. The PACF is a better indicator of the order of the AR model. The PACF of an AR(1) model is 0 for all lags after the first lag. The PACF of an AR(2) model is 0 for all lags after the second lag.*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot the ACF and the PACF of an AR, a MA, and ARMA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ar_sim = ArmaProcess(ar = [0.3, -0.25]).generate_sample(nsample=200)\n",
    "plt.plot(arima_sim)\n",
    "\n",
    "ma_sim = ArmaProcess(ma = [0.3, -0.25]).generate_sample(nsample=200)\n",
    "plt.plot(ma_sim)\n",
    "\n",
    "arma_sim = ArmaProcess(ar = [0.3, -0.25], ma = [0.3, -0.25]).generate_sample(nsample=200)\n",
    "plt.plot(arma_sim)\n",
    "\n",
    "plt.legend(['AR', 'MA', 'ARMA'])\n",
    "plt.show()\n",
    "\n",
    "plot_acf(ar_sim, lags=20).show()\n",
    "plot_acf(ma_sim, lags=20).show()\n",
    "plot_acf(arma_sim, lags=20).show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The main difference between the ACF and the PACF is that the PACF is a better indicator of the order of the AR model. The PACF of an AR(1) model is 0 for all lags after the first lag. The PACF of an AR(2) model is 0 for all lags after the second lag.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write the equation of an ARMA model\n",
    "\n",
    "$$X_t = \\mu + \\epsilon_t + \\sum_{i=1}^p \\phi_i X_{t-i} + \\sum_{i=1}^q \\theta_i \\epsilon_{t-i}$$\n",
    "\n",
    "6. What are the main differences between AIC and BIC criteria, conceptually speaking ? Elaborate (Min. 100 words).\n",
    "\n",
    "The AIC criterion is the log-likelihood of the model plus a penalty term. The BIC criterion is the log-likelihood of the model plus a penalty term. The penalty term is higher for the BIC criterion. The AIC criterion is more suitable for small sample sizes, while the BIC criterion is more suitable for large sample sizes.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forecasting competition on Kaggle\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose from the sales list the item, whose id is 20949."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the kaggle dataset\n",
    "sales_train = pd.read_csv('Kaggle dataset/sales_train.csv')\n",
    "sales_train[\"date\"] = pd.to_datetime(sales_train[\"date\"], format=\"%d.%m.%Y\")\n",
    "\n",
    "sales_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose item #20949 in sales_train\n",
    "sales_train_item20949 = sales_train[sales_train[\"item_id\"] == 20949]\n",
    "sales_train_item20949"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a time series of the Kaggle dataset, but with a lag 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train_item20949_lag1 = sales_train_item20949.shift(1)\n",
    "sales_train_item20949_lag1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. cbind the two datasets, and look at them using the head command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_merge = sales_train_item20949_lag1.iloc[1: , :].append([None])\n",
    "merged_with_lag = pd.concat([sales_train_item20949, to_merge], axis=1).iloc[:-1, :].drop(0, axis=1)\n",
    "merged_with_lag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. use the cor function to look at the datasets, and then the acf function,with a lag 1. Plot the acf with different logs to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(sales_train_item20949.corr(), annot=True)\n",
    "sales_train_item20949[\"item_cnt_day\"].plot()\n",
    "plt.show()\n",
    "plot_acf(sales_train_item20949[\"item_cnt_day\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index to date\n",
    "sales_train_item20949 = sales_train_item20949.set_index(\"date\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Fit and plot an auto-regressive model to the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(sales_train_item20949[\"item_cnt_day\"], order=(1, 0, 0))\n",
    "model_fit = model.fit()\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.plot_diagnostics()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What are the intercept and the innovation variance (sigma2) estimate ? What do these parameters mean ?\n",
    "\n",
    "The intercept is the mean of the time series and the innovation variance is the variance of the error term."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Predict the sales for a month after the end of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the predict function, and the forecast function.\n",
    "# Plot the results.\n",
    "X = sales_train_item20949[\"item_cnt_day\"].values\n",
    "ARIMA_model = ARIMA(X, order=(3, 0, 0))\n",
    "ARIMA_model_fit = ARIMA_model.fit()\n",
    "predictions = ARIMA_model_fit.forecast(steps=700)\n",
    "\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Fit and plot a moving average model, and print the estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(sales_train_item20949[\"item_cnt_day\"], order=(0, 0, 1))\n",
    "model_fit = model.fit()\n",
    "model_fit.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Make a 1 to 15 steps forecast, and plot the 95 percent confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sales_train_item20949[\"item_cnt_day\"].values\n",
    "ARIMA_model = ARIMA(X, order=(0, 0, 1))\n",
    "ARIMA_model_fit = ARIMA_model.fit()\n",
    "predictions = ARIMA_model_fit.get_forecast(steps=15)\n",
    "confidence_interval = pd.DataFrame(predictions.conf_int(alpha = 0.05))\n",
    "\n",
    "plt.plot(predictions.predicted_mean, color='red')\n",
    "plt.fill_between(confidence_interval.index, confidence_interval.iloc[:, 0], confidence_interval.iloc[:, 1], color='red', alpha=.25)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Compare the goodness of fit of AR and MA models through AIC and BIC criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR(1)\n",
    "model = ARIMA(sales_train_item20949[\"item_cnt_day\"], order=(1, 0, 0))\n",
    "model_fit = model.fit()\n",
    "model_fit.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MA(1)\n",
    "model = ARIMA(sales_train_item20949[\"item_cnt_day\"], order=(0, 0, 1))\n",
    "model_fit = model.fit()\n",
    "model_fit.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. What model performs the best according to you ?\n",
    "\n",
    "The best model in this case according to AIC and BIC is the AR model, because lower is better.\n",
    "\n",
    "12. Make the ACF plots for various lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(sales_train_item20949[\"item_cnt_day\"], lags=20).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Use the residual analysis graphics of the Sarima function to check whether there are patterns in the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a SARIMA model\n",
    "model = SARIMAX(sales_train_item20949[\"item_cnt_day\"], order=(1, 0, 0), seasonal_order=(1, 0, 0, 12))\n",
    "model_fit = model.fit(disp=False)\n",
    "model_fit.plot_diagnostics().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Do you see any patterns in the residuals for the diverse models you have implemented ?\n",
    "\n",
    "No, the residuals are white noise.\n",
    "\n",
    "15. How should Q-Q plot look like when the model is a good fit ?\n",
    "\n",
    "The points of the Q-Q plot should all follow the line, in this case, the points follow the line quite well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Fit various models (ARMA(1,1) - ARMA(2,1) - ARIMA(1,1,1) - ARIMA(1,1,0) to the Kaggle time series, and plot the t-table, check diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARMA(1,1)\n",
    "model = ARIMA(sales_train_item20949[\"item_cnt_day\"], order=(1, 0, 1))\n",
    "model_fit = model.fit()\n",
    "model_fit.plot_diagnostics().show()\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARMA(2,1)\n",
    "model = ARIMA(sales_train_item20949[\"item_cnt_day\"], order=(2, 0, 1))\n",
    "model_fit = model.fit()\n",
    "model_fit.plot_diagnostics().show()\n",
    "model_fit.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ARIMA(1,1,1)\n",
    "model = ARIMA(sales_train_item20949[\"item_cnt_day\"], order=(1, 1, 1))\n",
    "model_fit = model.fit()\n",
    "model_fit.plot_diagnostics().show()\n",
    "model_fit.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ARIMA(1,1,0)\n",
    "model = ARIMA(sales_train_item20949[\"item_cnt_day\"], order=(1, 1, 0))\n",
    "model_fit = model.fit()\n",
    "model_fit.plot_diagnostics().show()\n",
    "model_fit.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Fit a seasonal model to the Kaggle dataset. Fit ACF models with the relevant lags. Example : SARIMA(2,1,0,1,0,0,12). What are the conceptual differences between an ARIMA and a SARIMA, from the mathematical point of view ? Then, play with the parameters of the seasonnal component of the model (increase them), and see how forecasting is affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA(2,1,0,1,0,0,12)\n",
    "model = SARIMAX(sales_train_item20949[\"item_cnt_day\"], order=(2, 1, 0), seasonal_order=(1, 0, 0, 12))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())\n",
    "model_fit.plot_diagnostics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The difference between ARIMA and SARIMA is that the ARIMA model is a linear model, while the SARIMA model is a non-linear model. The SARIMA model is a linear model with a seasonal component.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Present a relevant sample of Ljung-Box tests, and explain how they can be used to assess your models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acorr_ljungbox(model_fit.resid, lags=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], boxpierce=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The Ljung-Box test is a test for autocorrelation. The null hypothesis is that the residuals are white noise. The p-value is the probability of rejecting the null hypothesis. If the p-value is lower than 0.05, we reject the null hypothesis and conclude that the residuals are not white noise.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. Use your favourite model to forecast the time series on the available test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a SARIMA model\n",
    "model = SARIMAX(sales_train_item20949[\"item_cnt_day\"], order=(1, 0, 0), seasonal_order=(1, 0, 0, 12))\n",
    "model_fit = model.fit(disp=False)\n",
    "model_fit.plot_diagnostics()\n",
    "\n",
    "# Make predictions and plot them using model_fit\n",
    "predictions = model_fit.get_forecast(steps=700)\n",
    "sarimax_predictions = predictions.predicted_mean\n",
    "confidence_interval = pd.DataFrame(predictions.conf_int(alpha = 0.05))\n",
    "\n",
    "plt.plot(predictions.predicted_mean, color='red')\n",
    "plt.fill_between(confidence_interval.index, confidence_interval.iloc[:, 0], confidence_interval.iloc[:, 1], color='red', alpha=.25)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Perform a naive forecast based on the forecast package, to predict the sales for the training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train_item20949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train_item20949.reset_index(inplace=True)\n",
    "# Fit a naive model\n",
    "naive_model = NaiveForecaster()\n",
    "naive_model.fit(y = sales_train_item20949[\"item_cnt_day\"], X = sales_train_item20949[\"date\"])\n",
    "\n",
    "# Make predictions and plot them using model_fit\n",
    "predictions = naive_model.predict(fh=list(range(30)))\n",
    "naive_predictions = predictions\n",
    "\n",
    "plt.plot(sales_train_item20949[\"item_cnt_day\"][-30:], color='blue')\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Remind us to what correspond the two shades of blue in the confidence intervals\n",
    "\n",
    "*The two shades of blue correspond to the 95% confidence interval.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. Use the accuracy command to compute the RMSE statistics of your favourite models. What does RMSE mean ? What is the difference with MAE ? Why is it usually preferred to MSE ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax_predictions[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy of the naive model, and compare it to the accuracy of the SARIMA model\n",
    "\n",
    "# Compute the accuracy of the naive model\n",
    "\n",
    "print(\"Naive model accuracy : \", mean_squared_error(sales_train_item20949[\"item_cnt_day\"][-30:], naive_predictions))\n",
    "\n",
    "# Compute the accuracy of the SARIMA model\n",
    "\n",
    "print(\"SARIMA model accuracy : \", mean_squared_error(sales_train_item20949[\"item_cnt_day\"][-30:], sarimax_predictions[-30:]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*From this we can conclude that a simple forecasting method is less accurate than a more complex one on the last month.*\n",
    "\n",
    "*RMSE is the root mean squared error. It is the square root of the mean of the squared errors. The difference with MAE is that the RMSE is more sensitive to outliers. It is usually preferred to MSE because it is easier to interpret.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. Write a command that only returns the mean absolute percentage error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. Compute cross-validated errors for up to a week ahead, with a naive forecast approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cross-validated errors for up to a week ahead, with a naive forecast approach\n",
    "\n",
    "naive_model = NaiveForecaster(strategy=\"mean\", sp=12)\n",
    "naive_model.fit(y = sales_train_item20949[\"item_cnt_day\"], X = sales_train_item20949[\"date\"])\n",
    "naive_predictions = naive_model.predict(fh=list(range(30)))\n",
    "\n",
    "print(\"Naive model accuracy : \", mean_absolute_percentage_error(sales_train_item20949[\"item_cnt_day\"][-30:], naive_predictions))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. Use Auto-ARIMA to fit a model to your data. How does Auto-ARIMA work ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_datetime = sales_train_item20949.groupby(\"date\").sum().reset_index()\n",
    "sales_datetime = sales_datetime[[\"date\", \"item_cnt_day\"]]\n",
    "sales_datetime = sales_datetime.set_index(\"date\")\n",
    "sales_datetime = sales_datetime.asfreq('D')\n",
    "sales_datetime = sales_datetime.fillna(0)\n",
    "sales_datetime[\"item_cnt_day\"] = sales_datetime[\"item_cnt_day\"].astype(int)\n",
    "sales_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Auto-ARIMA to fit a model to your data. How does Auto-ARIMA work ?\n",
    "\n",
    "# Fit an auto-ARIMA model\n",
    "auto_arima_model = AutoARIMA(seasonal=True)\n",
    "auto_arima_model.fit(sales_datetime[\"item_cnt_day\"])\n",
    "\n",
    "# Make predictions and plot them using model_fit\n",
    "predictions = auto_arima_model.predict(fh=list(range(30)))\n",
    "auto_arima_predictions = predictions\n",
    "\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Auto-arima works by trying different combinations of parameters and choosing the one that minimizes the AIC criterion.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Forecasting with Prophet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we are going to focus on applying prophet to a dataset heavily influenced by human activity: crimes in Chicago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import crimes data\n",
    "crimes = pd.read_csv(\"crimes.csv\")\n",
    "crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assaults = crimes[crimes[\"Primary Type\"] == \"ASSAULT\"]\n",
    "assaults.Date = pd.to_datetime(assaults.Date, format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "assaults[\"Date day\"] = assaults.Date.dt.date\n",
    "assaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions of the assaults with prophet\n",
    "assaults_prophet = assaults.groupby(\"Date day\").size().reset_index()\n",
    "assaults_prophet.columns = [\"ds\", \"y\"]\n",
    "assaults_prophet.ds = pd.to_datetime(assaults_prophet.ds)\n",
    "assaults_prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a prophet model\n",
    "assaults_prophet_model = Prophet()\n",
    "assaults_prophet_model.fit(assaults_prophet)\n",
    "\n",
    "# Make predictions and plot them using model_fit\n",
    "predictions = assaults_prophet_model.make_future_dataframe(periods=365)\n",
    "predictions = assaults_prophet_model.predict(predictions)\n",
    "assaults_prophet_predictions = predictions\n",
    "\n",
    "assaults_prophet_model.plot(predictions, figsize=(7,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assaults_prophet_model.plot_components(predictions, figsize=(7,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same prediction using a SARIMA\n",
    "\n",
    "# Fit a SARIMA model using AutoARIMA\n",
    "model = AutoARIMA(seasonal=True)\n",
    "model_fit = model.fit(assaults_prophet[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = model_fit.predict(fh=list(range(365)))\n",
    "conf_int = model_fit.predict_interval(fh=list(range(365)))\n",
    "sarima_predictions = pd.DataFrame(forecast, columns=[\"yhat\"])\n",
    "sarima_predictions[\"yhat_lower\"] = conf_int.iloc[:, 0]\n",
    "sarima_predictions[\"yhat_upper\"] = conf_int.iloc[:, 1]\n",
    "sarima_predictions.index = pd.date_range(start=assaults_prophet.ds.max(), periods=365)\n",
    "sarima_predictions.ds = pd.date_range(start=assaults_prophet.ds.max(), periods=365)\n",
    "\n",
    "\n",
    "# Plot the predictions\n",
    "plt.plot(assaults_prophet.set_index(\"ds\")[\"y\"], color='blue')\n",
    "plt.plot(sarima_predictions[\"yhat\"], color='red')\n",
    "# plt.fill_between(sarima_predictions.ds, sarima_predictions[\"yhat_lower\"], sarima_predictions[\"yhat_upper\"], color='red', alpha=.25)\n",
    "plt.plot(sarima_predictions[\"yhat_lower\"], color='red', alpha=.25)\n",
    "plt.plot(sarima_predictions[\"yhat_upper\"], color='red', alpha=.25)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discrete Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make f(t)=2+0.75×sin(3wt)+0.25×sin(7wt)+0.5×sin(10wt) into python\n",
    "\n",
    "# Create a function that returns the value of f(t) for a given t\n",
    "def f(t, w):\n",
    "    dc = 1\n",
    "\n",
    "    return dc + 0.75 * np.sin(3 * w * t) + 0.25 * np.sin(7 * w * t) + 0.5 * np.sin(10 * w * t)\n",
    "\n",
    "# Plot f\n",
    "t = np.linspace(0, np.pi, 1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make f(t)=2+0.75×sin(3wt)+0.25×sin(7wt)+0.5×sin(10wt) into python\n",
    "\n",
    "# Create a function that returns the value of f(t) for a given t\n",
    "def f(t, w):\n",
    "    dc = 1\n",
    "\n",
    "    return dc + 0.75 * np.sin(3 * w * t * np.pi) + 0.25 * np.sin(7 * w * t * np.pi) + 0.5 * np.sin(10 * w * t * np.pi)\n",
    "\n",
    "# Plot f\n",
    "t = np.linspace(0, 1, 1000)\n",
    "plt.plot(t, f(t, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the following R script into python\n",
    "# acq.freq <- 100                    # data acquisition (sample) frequency (Hz)\n",
    "# time     <- 6                      # measuring time interval (seconds)\n",
    "# ts       <- seq(0,time-1/acq.freq,1/acq.freq) # vector of sampling time-points (s) \n",
    "# f.0 <- 1/time\n",
    "\n",
    "# dc.component <- 1\n",
    "# component.freqs <- c(3,7,10)        # frequency of signal components (Hz)\n",
    "# component.delay <- c(0,0,0)         # delay of signal components (radians)\n",
    "# component.strength <- c(1.5,.5,.75) # strength of signal components\n",
    "\n",
    "# f   <- function(t,w) { \n",
    "#   dc.component + \n",
    "#   sum( component.strength * sin(component.freqs*w*t + component.delay)) \n",
    "# }\n",
    "\n",
    "# plot.fourier <- function(fourier.series, f.0, ts) {\n",
    "#   w <- 2*pi*f.0\n",
    "#   trajectory <- sapply(ts, function(t) fourier.series(t,w))\n",
    "#   plot(ts, trajectory, type=\"l\", xlab=\"time\", ylab=\"f(t)\"); abline(h=0,lty=3)\n",
    "# }\n",
    "\n",
    "# plot.fourier(f,f.0,ts=ts)\n",
    "\n",
    "\n",
    "# Python Version\n",
    "acq_freq = 100                    # data acquisition (sample) frequency (Hz)\n",
    "time = 6                      # measuring time interval (seconds)\n",
    "ts = np.linspace(0, time, time*acq_freq) # vector of sampling time-points (s)\n",
    "f_0 = 1/time\n",
    "\n",
    "dc_component = 1\n",
    "component_freqs = np.array([3,7,10])        # frequency of signal components (Hz)\n",
    "component_delay = np.array([0,0,0])         # delay of signal components (radians)\n",
    "component_strength = np.array([1.5,.5,.75]) # strength of signal components\n",
    "\n",
    "def f(t, w):\n",
    "    return dc_component + np.sum(component_strength * np.sin(component_freqs*w*t + component_delay))\n",
    "\n",
    "def plot_fourier(fourier_series, f_0, ts):\n",
    "    w = 2*np.pi*f_0\n",
    "    trajectory = np.array([fourier_series(t, w) for t in ts])\n",
    "    plt.plot(ts, trajectory)\n",
    "    plt.axhline(y=dc_component, color='r', linestyle='dashed')\n",
    "\n",
    "plot_fourier(f, f_0, ts=ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this R function to python\n",
    "# plot.frequency.spectrum <- function(X.k, xlimits=c(0,length(X.k))) {\n",
    "#   plot.data  <- cbind(0:(length(X.k)-1), Mod(X.k))\n",
    "\n",
    "#   # TODO: why this scaling is necessary?\n",
    "#   plot.data[2:length(X.k),2] <- 2*plot.data[2:length(X.k),2] \n",
    "  \n",
    "#   plot(plot.data, t=\"h\", lwd=2, main=\"\", \n",
    "#        xlab=\"Frequency (Hz)\", ylab=\"Strength\", \n",
    "#        xlim=xlimits, ylim=c(0,max(Mod(plot.data[,2]))))\n",
    "# }\n",
    "\n",
    "def plot_frequency_spectrum(X_k, xlimits=None):\n",
    "    if xlimits is None:\n",
    "        xlimits = [0, len(X_k) - 1]\n",
    "    plot_data = np.array([[i, np.abs(X_k[i])] for i in range(len(X_k))])\n",
    "    plot_data[1:, 1] = 2 * plot_data[1:, 1]\n",
    "    plt.bar(plot_data[:, 0], plot_data[:, 1])\n",
    "    plt.xlim(xlimits)\n",
    "    plt.ylim([0, np.max(plot_data[:, 1])])\n",
    "    plt.show()\n",
    "\n",
    "# Convert the rest of the R script\n",
    "# w <- 2*pi*f.0\n",
    "# trajectory <- sapply(ts, function(t) f(t,w))\n",
    "# X.k <- fft(trajectory)                   # find all harmonics with fft()\n",
    "# plot.frequency.spectrum(X.k, xlimits=c(0,20))\n",
    "\n",
    "w = 2*np.pi*f_0\n",
    "trajectory = np.array([f(t, w) for t in ts])\n",
    "\n",
    "X_k = np.fft.fft(trajectory)\n",
    "plot_frequency_spectrum(X_k, xlimits=[0, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert function to python\n",
    "# get.trajectory <- function(X.k,ts,acq.freq) {\n",
    "  \n",
    "#   N   <- length(ts)\n",
    "#   i   <- complex(real = 0, imaginary = 1)\n",
    "#   x.n <- rep(0,N)           # create vector to keep the trajectory\n",
    "#   ks  <- 0:(length(X.k)-1)\n",
    "  \n",
    "#   for(n in 0:(N-1)) {       # compute each time point x_n based on freqs X.k\n",
    "#     x.n[n+1] <- sum(X.k * exp(i*2*pi*ks*n/N)) / N\n",
    "#   }\n",
    "  \n",
    "#   x.n * acq.freq \n",
    "# }\n",
    "\n",
    "def get_trajectory(X_k, ts, acq_freq):\n",
    "    N = len(ts)\n",
    "    i = complex(0, 1)\n",
    "    x_n = np.zeros(N)\n",
    "    ks = np.arange(len(X_k))\n",
    "    \n",
    "    for n in range(N):\n",
    "        x_n[n] = np.sum(X_k * np.exp(i*2*np.pi*ks*n/N)) / N\n",
    "\n",
    "    return x_n * acq_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R to python\n",
    "# x.n <- get.trajectory(X.k,ts,acq.freq) / acq.freq  # TODO: why the scaling?\n",
    "# plot(ts,x.n, type=\"l\"); abline(h=0,lty=3)\n",
    "# points(ts,trajectory,col=\"red\",type=\"l\") # compare with original\n",
    "\n",
    "x_n = get_trajectory(X_k, ts, acq_freq) / acq_freq\n",
    "plt.plot(ts, x_n, color='blue')\n",
    "plt.plot(ts, trajectory, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert R to python\n",
    "# plot.harmonic <- function(Xk, i, ts, acq.freq, color=\"red\") {\n",
    "#   Xk.h <- rep(0,length(Xk))\n",
    "#   Xk.h[i+1] <- Xk[i+1] # i-th harmonic\n",
    "#   harmonic.trajectory <- get.trajectory(Xk.h, ts, acq.freq=acq.freq)\n",
    "#   points(ts, harmonic.trajectory, type=\"l\", col=color)\n",
    "# }\n",
    "\n",
    "def plot_harmonic(X_k, i, ts, acq_freq, color='red'):\n",
    "    colors = ['red', 'green', 'blue', 'yellow', 'black', 'purple']\n",
    "    if type(color) == int:\n",
    "        color = colors[color % len(colors)]\n",
    "    X_k_h = np.zeros(len(X_k))\n",
    "    X_k_h[i] = X_k[i]\n",
    "    harmonic_trajectory = get_trajectory(X_k_h, ts, acq_freq)\n",
    "    plt.plot(ts, harmonic_trajectory, color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert R to python\n",
    "# plot.show <- function(trajectory, time=1, harmonics=-1, plot.freq=FALSE) {\n",
    "\n",
    "#   acq.freq <- length(trajectory)/time      # data acquisition frequency (Hz)\n",
    "#   ts  <- seq(0,time-1/acq.freq,1/acq.freq) # vector of sampling time-points (s) \n",
    "  \n",
    "#   X.k <- fft(trajectory)\n",
    "#   x.n <- get.trajectory(X.k,ts, acq.freq=acq.freq) / acq.freq\n",
    "  \n",
    "#   if (plot.freq)\n",
    "#     plot.frequency.spectrum(X.k)\n",
    "  \n",
    "#   max.y <- ceiling(1.5*max(Mod(x.n)))\n",
    "  \n",
    "#   if (harmonics[1]==-1) {\n",
    "#     min.y <- floor(min(Mod(x.n)))-1\n",
    "#   } else {\n",
    "#     min.y <- ceiling(-1.5*max(Mod(x.n)))\n",
    "#   }\n",
    "  \n",
    "#   plot(ts,x.n, type=\"l\",ylim=c(min.y,max.y))\n",
    "#   abline(h=min.y:max.y,v=0:time,lty=3)\n",
    "#   points(ts,trajectory,pch=19,col=\"red\")  # the data points we know\n",
    "  \n",
    "#   if (harmonics[1]>-1) {\n",
    "#     for(i in 0:length(harmonics)) {\n",
    "#       plot.harmonic(X.k, harmonics[i], ts, acq.freq, color=i+1)\n",
    "#     }\n",
    "#   }\n",
    "# }\n",
    "\n",
    "def plot_show(trajectory, time=1, harmonics=[-1], plot_freq=False):\n",
    "    acq_freq = len(trajectory) / time\n",
    "    ts = np.arange(0, time, 1/acq_freq)\n",
    "    \n",
    "    X_k = np.fft.fft(trajectory)\n",
    "    x_n = get_trajectory(X_k, ts, acq_freq) / acq_freq\n",
    "    \n",
    "    if plot_freq:\n",
    "        plot_frequency_spectrum(X_k)\n",
    "\n",
    "    max_y = np.ceil(1.5 * np.max(np.abs(x_n)))\n",
    "    \n",
    "    if harmonics[0] == -1:\n",
    "        min_y = np.floor(np.min(np.abs(x_n))) - 1\n",
    "    else:\n",
    "        min_y = np.ceil(-1.5 * np.max(np.abs(x_n)))\n",
    "    \n",
    "    plt.plot(ts, x_n, color='blue')\n",
    "    plt.axhline(y=min_y, color='red', linestyle='dashed')\n",
    "    plt.axhline(y=max_y, color='red', linestyle='dashed')\n",
    "    plt.axvline(x=0, color='red', linestyle='dashed')\n",
    "    plt.axvline(x=time-1/acq_freq, color='red', linestyle='dashed')\n",
    "    plt.scatter(ts, trajectory, color='red')\n",
    "\n",
    "    if harmonics[0] > -1:\n",
    "        for i in range(len(harmonics)):\n",
    "            # fix the problem with the first harmonic\n",
    "            if harmonics[i] != 0:\n",
    "                plot_harmonic(X_k, harmonics[i], ts, acq_freq, color=i)\n",
    "            else :\n",
    "                plt.plot(ts, np.zeros(len(ts)), color=\"red\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R to python\n",
    "# trajectory <- 4:1\n",
    "# plot_show(trajectory, time=2)\n",
    "\n",
    "trajectory = np.arange(4, 0, -1)\n",
    "plot_show(trajectory, time=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R to python\n",
    "# trajectory <- c(rep(1,5),rep(2,6),rep(3,7))\n",
    "# plot.show(trajectory, time=2, harmonics=0:3, plot.freq=TRUE)\n",
    "\n",
    "trajectory = np.concatenate((np.ones(5), np.ones(6)*2, np.ones(7)*3))\n",
    "plot_show(trajectory, time=2, harmonics=[0, 1, 2, 3], plot_freq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R to python\n",
    "# trajectory <- c(1:5,2:6,3:7)\n",
    "# plot.show(trajectory, time=1, harmonics=c(1,2))\n",
    "\n",
    "trajectory = np.concatenate((np.arange(1, 6), np.arange(2, 7), np.arange(3, 8)))\n",
    "plot_show(trajectory, time=1, harmonics=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R to python\n",
    "# set.seed(101)\n",
    "# acq.freq <- 200\n",
    "# time     <- 1\n",
    "# w        <- 2*pi/time\n",
    "# ts       <- seq(0,time,1/acq.freq)\n",
    "# trajectory <- 3*rnorm(101) + 3*sin(3*w*ts)\n",
    "# plot(trajectory, type=\"l\")\n",
    "\n",
    "np.random.seed(101)\n",
    "acq_freq = 200\n",
    "time = 1\n",
    "w = 2 * np.pi / time\n",
    "ts = np.arange(0, time, 1/acq_freq)\n",
    "trajectory = 3 * np.random.randn(acq_freq) + 3 * np.sin(3 * w * ts)\n",
    "plt.plot(trajectory, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R to python\n",
    "# X.k <- fft(trajectory)\n",
    "# plot.frequency.spectrum(X.k,xlimits=c(0,acq.freq/2))\n",
    "\n",
    "X_k = np.fft.fft(trajectory)\n",
    "plot_frequency_spectrum(X_k, xlimits=[0, acq_freq/2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = trajectory + 25*ts\n",
    "plt.plot(trajectory, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = smf.ols('trajectory ~ ts', data={'trajectory': trajectory, 'ts': ts}).fit()\n",
    "detrended_trajectory = trend.resid\n",
    "plt.plot(detrended_trajectory, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From shapelet mining to discord detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain in 150 to 250 words what computing the matrix profile means. How\n",
    "does it work ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the matrix profile means using a mathematical algorithm to identify patterns and trends in time series data. This is accomplished by creating a matrix from the time series data, with each column representing a different time point and each row representing a different time series.\n",
    "\n",
    "The algorithm then calculates the similarity between each pair of columns in the matrix, and uses this information to identify patterns and trends in the data. This can include identifying \"discordant\" data points, which are points that do not fit the overall pattern of the data, and creating a \"profile\" of the data, which is a summary of the overall pattern of the data.\n",
    "\n",
    "The matrix profile algorithm is useful for quickly and easily identifying patterns and trends in time series data, without having to manually inspect every time point individually. This can help data scientists to better understand and interpret the data, and to identify potential outliers or issues with the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Motif detection\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the main parameters that you can play with during the motif detectionphase ? What happens when you change their value ? Elaborate (between 150 to 250 words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the motif detection phase, there are several main parameters that can be adjusted to change the behavior of the algorithm. These include:\n",
    "\n",
    "1. The length of the motif: This parameter determines the length of the motifs that the algorithm will be searching for in the data. Increasing the length of the motif will typically result in more specific and accurate results, but will also require more computational resources. Changing this parameter will affect the granularity of the motifs that are identified, with longer motifs being more specific and shorter motifs being more general.\n",
    "1. The distance metric: This parameter determines the mathematical formula that will be used to calculate the similarity between different time series. Different distance metrics will result in different results, and choosing the right metric can be critical for accurately identifying motifs in the data. Changing this parameter will affect the results of the motif detection phase, with different metrics producing different sets of motifs.\n",
    "1. The number of motifs: This parameter determines the number of motifs that the algorithm will be searching for in the data. Increasing the number of motifs will typically result in more detailed and accurate results, but will also require more computational resources. Changing this parameter will affect the number of motifs that are identified, with more motifs resulting in a more detailed analysis of the data.\n",
    "1. The minimum support: This parameter determines the minimum number of occurrences that a motif must have in order to be considered significant. Increasing the minimum support will typically result in more specific and accurate results, but will also reduce the number of motifs that are identified. Changing this parameter will affect the number of motifs that are identified, with higher minimum support levels resulting in fewer, but more significant, motifs.\n",
    "\n",
    "Overall, these parameters can be adjusted to change the behavior of the algorithm and to fine-tune the results of the motif detection phase. By carefully adjusting these parameters, analysts can optimize the algorithm for their specific data and analysis goals."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the code provided by the instructor, apply it to the Motif detection dataset to identify a set of motifs, that you will highlight using colours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code given by the instructor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matrixprofile_ts import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('rawdata.csv')\n",
    "pattern = data.data.values\n",
    "\n",
    "#Plot data\n",
    "fig, ax1 = plt.subplots(figsize=(20,5))\n",
    "ax1.plot(np.arange(len(pattern)),pattern, label=\"Synthetic Data\")\n",
    "legend = ax1.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=32\n",
    "mp = matrixProfile.stomp(pattern,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_motifs(mtfs, labels, ax):\n",
    "\n",
    "    colori = 0\n",
    "    colors = 'rgbcm'\n",
    "    for ms,l in zip(mtfs,labels):\n",
    "        c =colors[colori % len(colors)]\n",
    "        starts = list(ms)\n",
    "        ends = [min(s + m,len(pattern)-1) for s in starts]\n",
    "        ax.plot(starts, pattern[starts],  c +'o',  label=l)\n",
    "        ax.plot(ends, pattern[ends],  c +'o', markerfacecolor='none')\n",
    "        for nn in ms:\n",
    "            ax.plot(range(nn,nn+m),pattern[nn:nn+m], c , linewidth=2)\n",
    "        colori += 1\n",
    "\n",
    "    ax.plot(pattern, 'k', linewidth=1, label=\"data\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtfs ,motif_d  = motifs.motifs(pattern, mp, max_motifs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append np.nan to Matrix profile to enable plotting against raw data\n",
    "mp_adj = np.append(mp[0],np.zeros(m-1)+np.nan)\n",
    "\n",
    "#Plot the signal data\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3,1,sharex=True,figsize=(20,10))\n",
    "ax1.plot(np.arange(len(pattern)),pattern, label=\"Synthetic Data\")\n",
    "ax1.set_ylabel('Signal', size=22)\n",
    "\n",
    "#Plot the Matrix Profile\n",
    "ax2.plot(np.arange(len(mp_adj)),mp_adj, label=\"Matrix Profile\", color='red')\n",
    "ax2.set_ylabel('Matrix Profile', size=22)\n",
    "\n",
    "#Plot the Motifs\n",
    "plot_motifs(mtfs, [f\"{md:.3f}\" for md in motif_d], ax3)\n",
    "ax3.set_ylabel('Motifs', size=22)\n",
    "#plt.xlim((0,100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3,1,sharex=True,figsize=(20,10))\n",
    "\n",
    "\n",
    "mtfs ,motif_d  = motifs.motifs(pattern, mp, max_motifs=5, n_neighbors=4)\n",
    "plot_motifs(mtfs, [f\"{md:.3f}\" for md in motif_d], ax1)\n",
    "ax1.set_ylabel('4 Neigbhors', size=22)\n",
    "\n",
    "mtfs ,motif_d  = motifs.motifs(pattern, mp, max_motifs=5, radius=10)\n",
    "plot_motifs(mtfs, [f\"{md:.3f}\" for md in motif_d], ax2)\n",
    "ax2.set_ylabel('Radius = 10', size=22)\n",
    "\n",
    "mtfs ,motif_d  = motifs.motifs(pattern, mp, max_motifs=5, ex_zone=2*m)\n",
    "plot_motifs(mtfs, [f\"{md:.3f}\" for md in motif_d], ax3)\n",
    "ax3.set_ylabel('Exclude 2*m', size=22)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: NYC Taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matrixprofile as mp\n",
    "\n",
    "# ignore matplotlib warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('nyc_taxi.csv', parse_dates=[\"timestamp\"]).rename(columns={'timestamp': 'datetime', 'value': 'data'})\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,3))\n",
    "plt.plot(dataset['datetime'], dataset['data'])\n",
    "plt.title('NYC Taxi Passenger Counts')\n",
    "plt.ylabel('Passenger Count')\n",
    "plt.xlabel('Datetime')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imagine applications of motif detection in 5 different activity sectors (health, etc.). Min. 200 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many potential applications of motif detection in a variety of different activity sectors. Some examples include:\n",
    "\n",
    "1. Health: Motif detection could be used to identify patterns and trends in medical data, such as heart rate or blood pressure readings. This could help doctors to identify potential health issues, such as arrhythmias or hypertension, and to monitor patients for potential changes in their health status.\n",
    "1. Finance: Motif detection could be used to identify patterns and trends in financial data, such as stock prices or currency exchange rates. This could help investors to identify potential opportunities, such as buying low and selling high, and to avoid potential risks, such as market crashes or fraud.\n",
    "1. Agriculture: Motif detection could be used to identify patterns and trends in agricultural data, such as crop yields or weather patterns. This could help farmers to optimize their operations, such as by planting at the right times and using the right fertilizers, and to adapt to changing conditions, such as droughts or pests.\n",
    "1. Energy: Motif detection could be used to identify patterns and trends in energy data, such as electricity usage or renewable energy generation. This could help utilities to optimize their operations, such as by balancing supply and demand, and to adapt to changing conditions, such as increased demand or new technologies.\n",
    "1. Education: Motif detection could be used to identify patterns and trends in educational data, such as student performance or teacher effectiveness. This could help educators to identify potential issues, such as low test scores or high dropout rates, and to develop strategies for improvement, such as targeted interventions or professional development.\n",
    "\n",
    "\n",
    "Overall, motif detection has many potential applications in a wide range of activity sectors, and can be a valuable tool for identifying patterns and trends in data. By using motif detection, organizations in these sectors can better understand their data and use it to make informed decisions and take effective actions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Discord detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the code provided by the instructor, compute the matrix profile for New York City taxi dataset and provide a list of five dates between 2014 and 2015 for which a discord was detected. Interpret these discords after an online search, with a different interpretation for each date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the matrix profile technique again to find discords in an ECG. What could be the different applications of such a technique ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix profile technique could be used to find discords in an ECG (electrocardiogram) by creating a matrix from the ECG data, with each column representing a different time point and each row representing a different ECG waveform. The algorithm would then calculate the similarity between each pair of columns in the matrix, and use this information to identify discordant data points, which are points that do not fit the overall pattern of the data.\n",
    "\n",
    "There are several potential applications for this technique, including:\n",
    "\n",
    "1. Identifying potential health issues: By identifying discordant data points in an ECG, the matrix profile technique could be used to identify potential health issues, such as arrhythmias or heart attacks. This could be useful for doctors and other healthcare professionals, who could use the information to monitor patients and to take appropriate actions.\n",
    "1. Detecting abnormalities in ECG data: The matrix profile technique could also be used to detect abnormalities in ECG data, such as spikes or dips in the waveform. This could be useful for researchers and other scientists, who could use the information to study the underlying causes of these abnormalities and to develop potential treatments.\n",
    "1. Comparing ECG data to normal patterns: The matrix profile technique could also be used to compare ECG data to normal patterns, such as the average ECG waveform for a given age or gender. This could be useful for doctors and other healthcare professionals, who could use the information to assess the health of their patients and to identify potential issues.\n",
    "\n",
    "Overall, the matrix profile technique could be a valuable tool for identifying and analyzing patterns in ECG data, and could have many different applications in the field of healthcare and medical research. By using the technique, doctors and other healthcare professionals could better understand their patients' ECG data, and use it to make more informed decisions and take more effective actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Mel Spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "filename = 'Haunting_song_of_humpback_whales.wav'\n",
    "y, sr = librosa.load(filename)\n",
    "# trim silent edges\n",
    "whale_song, _ = librosa.effects.trim(y)\n",
    "librosa.display.waveshow(whale_song, sr=sr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_fft = 2048\n",
    "D = np.abs(librosa.stft(whale_song[:n_fft], n_fft=n_fft, hop_length=n_fft+1))\n",
    "plt.plot(D)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_length = 512\n",
    "D = np.abs(librosa.stft(whale_song, n_fft=n_fft,  hop_length=hop_length))\n",
    "librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='linear')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = librosa.amplitude_to_db(D, ref=np.max)\n",
    "librosa.display.specshow(DB, sr=sr, hop_length=hop_length, x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 128\n",
    "mel = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "librosa.display.specshow(mel, sr=sr, hop_length=hop_length, x_axis='linear')\n",
    "plt.ylabel('Mel filter')\n",
    "plt.colorbar()\n",
    "plt.title('1. Our filter bank for converting from Hz to mels.')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "mel_10 = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=10)\n",
    "librosa.display.specshow(mel_10, sr=sr, hop_length=hop_length, x_axis='linear')\n",
    "plt.ylabel('Mel filter')\n",
    "plt.colorbar()\n",
    "plt.title('2. Easier to see what is happening with only 10 mels.')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "idxs_to_plot = [0, 9, 49, 99, 127]\n",
    "for i in idxs_to_plot:\n",
    "    plt.plot(mel[i])\n",
    "plt.legend(labels=[f'{i+1}' for i in idxs_to_plot])\n",
    "plt.title('3. Plotting some triangular filters separately.')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(D[:, 1])\n",
    "plt.plot(mel.dot(D[:, 1]))\n",
    "plt.legend(labels=['Hz', 'mel'])\n",
    "plt.title('One sampled window for example, before and after converting to mel.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(whale_song, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "S_DB = librosa.power_to_db(S, ref=np.max)\n",
    "librosa.display.specshow(S_DB, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
